{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e54056c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson\n",
    "import json\n",
    "import uuid\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f16c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_json_file(filename, target_size_mb):\n",
    "    \"\"\"\n",
    "    Generates a JSON file of approximately target_size_mb.\n",
    "    Uses a streaming approach to keep memory usage low.\n",
    "    \"\"\"\n",
    "    target_bytes = target_size_mb * 1024 * 1024\n",
    "    roles = ['admin', 'user', 'editor', 'guest', 'support']\n",
    "    \n",
    "    print(f\"Generating {filename} ({target_size_mb} MB)...\")\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write('[\\n')  # Start JSON array\n",
    "        \n",
    "        current_size = f.tell()\n",
    "        first_record = True\n",
    "        record_count = 0\n",
    "\n",
    "        while current_size < target_bytes:\n",
    "            # Generate a \"CRUD\" style database record\n",
    "            record = {\n",
    "                \"id\": record_count,\n",
    "                \"uuid\": str(uuid.uuid4()),\n",
    "                \"username\": f\"user_name_{record_count}\",\n",
    "                \"email\": f\"contact_{record_count}@example-domain.com\",\n",
    "                \"profile\": {\n",
    "                    \"first_name\": random.choice(['John', 'Jane', 'Alex', 'Max', 'Sarah']),\n",
    "                    \"last_name\": random.choice(['Smith', 'Doe', 'Johnson', 'Brown', 'Lee']),\n",
    "                    \"bio\": \"This is a repeating string used to fill up space and simulate a longer text field in a database. \" * 3,\n",
    "                },\n",
    "                \"role\": random.choice(roles),\n",
    "                \"is_active\": random.choice([True, False]),\n",
    "                \"permissions\": [\"read\", \"write\", \"delete\"] if record_count % 10 == 0 else [\"read\"],\n",
    "                \"created_at\": \"2023-10-27T10:00:00Z\",\n",
    "                \"updated_at\": \"2024-01-15T14:30:22Z\"\n",
    "            }\n",
    "\n",
    "            # Handle comma placement for valid JSON\n",
    "            json_str = json.dumps(record, indent=2)\n",
    "            if not first_record:\n",
    "                f.write(',\\n')\n",
    "            \n",
    "            f.write(json_str)\n",
    "            \n",
    "            first_record = False\n",
    "            record_count += 1\n",
    "            \n",
    "            # Update current file size\n",
    "            # We flush occasionally to get an accurate file size check\n",
    "            if record_count % 100 == 0:\n",
    "                f.flush()\n",
    "                current_size = os.path.getsize(filename)\n",
    "\n",
    "        f.write('\\n]')  # End JSON array\n",
    "\n",
    "    final_size = os.path.getsize(filename) / (1024 * 1024)\n",
    "    print(f\"Finished! Actual size: {final_size:.2f} MB | Total Records: {record_count}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197bc030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating large_data.json (5000 MB)...\n",
      "Finished! Actual size: 5000.05 MB | Total Records: 7405800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generate_json_file('large_data.json', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd125919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID: 9516\n",
      "json streaming took 709.48 seconds | objects parsed: 14,786,500\n",
      "orjson streaming took 694.89 seconds | objects parsed: 14,786,500\n"
     ]
    }
   ],
   "source": [
    "# json vs orjson reading benchmarks (memory-aware)\n",
    "import time\n",
    "import gc\n",
    "import os\n",
    "\n",
    "pid = os.getpid()\n",
    "print(f\"Process ID: {pid}\")\n",
    "\n",
    "def benchmark_json_full_read(filename):\n",
    "    \"\"\"Full parse with stdlib json (high memory).\"\"\"\n",
    "    start_time = time.time()\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"json.load (full) took {elapsed:.2f} seconds | records: {len(data):,}\")\n",
    "\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "def benchmark_orjson_full_read(filename):\n",
    "    \"\"\"Full parse with orjson (high memory).\"\"\"\n",
    "    start_time = time.time()\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = orjson.loads(f.read())\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"orjson.loads (full) took {elapsed:.2f} seconds | records: {len(data):,}\")\n",
    "\n",
    "    del data\n",
    "    gc.collect()\n",
    "\n",
    "def iter_json_array_objects(filename, chunk_size_mb=8):\n",
    "    \"\"\"\n",
    "    Stream top-level objects from a JSON array file as raw bytes.\n",
    "    Keeps memory low by not loading the whole file.\n",
    "    Assumes top-level is: [ { ... }, { ... }, ... ]\n",
    "    \"\"\"\n",
    "    chunk_size = chunk_size_mb * 1024 * 1024\n",
    "    in_string = False\n",
    "    escape = False\n",
    "    brace_depth = 0\n",
    "    collecting = False\n",
    "    obj_buffer = bytearray()\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "\n",
    "            for b in chunk:\n",
    "                if not collecting:\n",
    "                    # Wait until the start of the next object\n",
    "                    if b == ord('{'):\n",
    "                        collecting = True\n",
    "                        brace_depth = 1\n",
    "                        obj_buffer = bytearray([b])\n",
    "                    continue\n",
    "\n",
    "                obj_buffer.append(b)\n",
    "\n",
    "                if in_string:\n",
    "                    if escape:\n",
    "                        escape = False\n",
    "                    elif b == ord('\\\\'):\n",
    "                        escape = True\n",
    "                    elif b == ord('\"'):\n",
    "                        in_string = False\n",
    "                    continue\n",
    "\n",
    "                if b == ord('\"'):\n",
    "                    in_string = True\n",
    "                elif b == ord('{'):\n",
    "                    brace_depth += 1\n",
    "                elif b == ord('}'):\n",
    "                    brace_depth -= 1\n",
    "                    if brace_depth == 0:\n",
    "                        yield bytes(obj_buffer)\n",
    "                        collecting = False\n",
    "                        obj_buffer = bytearray()\n",
    "\n",
    "def benchmark_json_streaming_read(filename, chunk_size_mb=8, limit=None):\n",
    "    \"\"\"\n",
    "    Streaming parse using json.loads per object.\n",
    "    Low memory; suitable for huge JSON arrays.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    count = 0\n",
    "\n",
    "    for obj_bytes in iter_json_array_objects(filename, chunk_size_mb=chunk_size_mb):\n",
    "        _ = json.loads(obj_bytes.decode('utf-8'))\n",
    "        count += 1\n",
    "        if limit is not None and count >= limit:\n",
    "            break\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"json streaming took {elapsed:.2f} seconds | objects parsed: {count:,}\")\n",
    "    gc.collect()\n",
    "\n",
    "def benchmark_orjson_streaming_read(filename, chunk_size_mb=8, limit=None):\n",
    "    \"\"\"\n",
    "    Streaming parse using orjson.loads per object.\n",
    "    Low memory; suitable for huge JSON arrays.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    count = 0\n",
    "\n",
    "    for obj_bytes in iter_json_array_objects(filename, chunk_size_mb=chunk_size_mb):\n",
    "        _ = orjson.loads(obj_bytes)\n",
    "        count += 1\n",
    "        if limit is not None and count >= limit:\n",
    "            break\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"orjson streaming took {elapsed:.2f} seconds | objects parsed: {count:,}\")\n",
    "    gc.collect()\n",
    "\n",
    "# RECOMMENDED for very large files (low memory):\n",
    "benchmark_json_streaming_read('large_data.json', chunk_size_mb=8)\n",
    "benchmark_orjson_streaming_read('large_data.json', chunk_size_mb=8)\n",
    "\n",
    "# Optional full-memory benchmarks (may OOM on very large files):\n",
    "# benchmark_json_full_read('large_data.json')\n",
    "# benchmark_orjson_full_read('large_data.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4575-sustainable-software-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
